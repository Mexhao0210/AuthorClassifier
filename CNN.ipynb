{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Manu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\manu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "c:\\users\\manu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Embedding, Conv2D, MaxPooling2D, Reshape, concatenate, Flatten, Dropout, Dense\n",
    "from keras import regularizers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "Y = train_df['author']\n",
    "X = train_df.drop(columns=['author'])\n",
    "\n",
    "del train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(sent):\n",
    "    sent = str(sent)\n",
    "    sent = sent.lower()\n",
    "    \n",
    "    sent = re.sub(r\"what's\", \"what is \", sent)\n",
    "    sent = re.sub(r\"\\'s\", \" \", sent)\n",
    "    sent = re.sub(r\"\\'ve\", \" have \", sent)\n",
    "    sent = re.sub(r\"can't\", \"cannot \", sent)\n",
    "    sent = re.sub(r\"n't\", \" not \", sent)\n",
    "    sent = re.sub(r\"i'm\", \"i am \", sent)\n",
    "    sent = re.sub(r\"\\'re\", \" are \", sent)\n",
    "    sent = re.sub(r\"\\'d\", \" would \", sent)\n",
    "    sent = re.sub(r\"\\'ll\", \" will \", sent)\n",
    "    sent = re.sub(r\",\", \" \", sent)\n",
    "    sent = re.sub(r\"\\.\", \" \", sent)\n",
    "    sent = re.sub(r\"!\", \" ! \", sent)\n",
    "    sent = re.sub(r\"\\/\", \" \", sent)\n",
    "    sent = re.sub(r\"\\^\", \" ^ \", sent)\n",
    "    sent = re.sub(r\"\\+\", \" + \", sent)\n",
    "    sent = re.sub(r\"\\-\", \" - \", sent)\n",
    "    sent = re.sub(r\"\\=\", \" = \", sent)\n",
    "    sent = re.sub(r\"'\", \" \", sent)\n",
    "    \n",
    "    sent = sent.split()\n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\manu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:24: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "vocab = dict()\n",
    "inverse_vocab = ['<UNK>']\n",
    "#embedding_file = \"GoogleNews-vectors-negative300.bin.gz\";\n",
    "glove_embedding = \"glove.6B.50d.txt\"\n",
    "embedding_file = \"glove_50d.txt\"\n",
    "\n",
    "glove2word2vec(glove_embedding, embedding_file)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word2vec_embeddings = KeyedVectors.load_word2vec_format(embedding_file)\n",
    "\n",
    "for index, row in X.iterrows():\n",
    "    token_vector = []\n",
    "    for word in preprocess_text(row['text']):\n",
    "        if word in stop_words and word not in word2vec_embeddings.vocab:\n",
    "            continue\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(inverse_vocab)\n",
    "            token_vector.append(len(inverse_vocab))\n",
    "            inverse_vocab.append(word)\n",
    "        else:\n",
    "            token_vector.append(vocab[word])\n",
    "    X.set_value(index, 'text', token_vector)\n",
    "    \n",
    "embed_dim = 50\n",
    "embeddings = 1 * np.random.randn(len(vocab) + 1, embed_dim)  # This will be the embedding matrix\n",
    "embeddings[0] = 0 \n",
    "\n",
    "for word, index in vocab.items():\n",
    "    if word in word2vec_embeddings.vocab:\n",
    "        embeddings[index] = word2vec_embeddings.word_vec(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sent_length = X.text.map(lambda x: len(x)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_values = X.text\n",
    "X_values = pad_sequences(X_values, maxlen=max_sent_length)\n",
    "\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_values, Y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(len(embeddings), embed_dim, weights=[embeddings], input_length=max_sent_length, trainable=True)\n",
    "model.add(embedding_layer)\n",
    "model.add(Reshape((max_sent_length, embed_dim, 1)))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(10, embed_dim), activation=\"relu\", padding=\"same\", name=\"Conv1\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), name=\"Pool1\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation=\"softmax\", name=\"Dense1\"))\n",
    "\n",
    "rmsOpt = RMSprop()\n",
    "model.compile(optimizer=rmsOpt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 1000\n",
    "\n",
    "model.fit(x=X_train, y=Y_train, epochs=epochs, batch_size=batch_size, validation_data=[X_test, Y_test], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
